{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "050da67e-f709-4e8d-af63-49ec244616a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting some initial params here\n",
    "image_size = (224, 224)\n",
    "save_dir = f\"{image_size}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54f6fa4-6a45-42b7-8097-b2202fe06e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "num_epochs = 30\n",
    "batch_size = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "# we need to normalize\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_dataset = ImageFolder(root=r\"C:\\Users\\computer\\Downloads\\cosoco\\dataset\\train\", transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=30, pin_memory=True)\n",
    "\n",
    "classes = train_dataset.classes\n",
    "print(\"Detected classes:\", classes)\n",
    "\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)\n",
    "model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total_batches = len(train_loader)\n",
    "\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Print every 10 batches\n",
    "        if (i + 1) % 10 == 0 or (i + 1) == total_batches:\n",
    "            avg_loss = running_loss / (i + 1)\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}/{total_batches}], Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] finished. Average Loss: {running_loss / total_batches:.4f}\")\n",
    "\n",
    "    # Save the model every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        save_path = os.path.join(save_dir, f\"kernel_model_weights_epoch_{epoch+1}.pth\")\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(f\"Model saved to {save_path}\")\n",
    "\n",
    "final_save_path = os.path.join(save_dir, f\"kernel_model_weights_epoch_{num_epochs}.pth\")\n",
    "torch.save(model.state_dict(), final_save_path)\n",
    "print(f\"Final model saved to {final_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2a0e10-d4e5-421a-95fa-b3ccfb675340",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "test_dataset = ImageFolder(root=r\"C:\\Users\\computer\\Downloads\\cosoco\\dataset\\test\", transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=30, pin_memory=True)\n",
    "classes = test_dataset.classes\n",
    "\n",
    "# Evaluate models from epoch 5 to 30 inclusive\n",
    "for epoch in range(5, 16, 5):\n",
    "    model = models.resnet18(pretrained=False)\n",
    "    model.fc = nn.Linear(model.fc.in_features, 2)\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride = 1, padding = 1, bias = False)\n",
    "    model.load_state_dict(torch.load(f\"{save_dir}\\\\kernel_model_weights_epoch_{epoch}.pth\", map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100.0 * correct / total\n",
    "    print(f\"Accuracy of model at epoch {epoch}: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5b88a8-7690-407e-abaf-d9e99c58c5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# transform to match resnet18 mean/std\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dir = r\"C:\\Users\\computer\\Downloads\\cosoco\\dataset\\train\"\n",
    "test_dir = r\"C:\\Users\\computer\\Downloads\\cosoco\\dataset\\test\"\n",
    "train_set = ImageFolder(root=train_dir, transform=transform)\n",
    "test_set = ImageFolder(root=test_dir, transform=transform)\n",
    "train_loader = DataLoader(train_set, batch_size=80, shuffle=True, num_workers=15)\n",
    "test_loader = DataLoader(test_set, batch_size=80, shuffle=False, num_workers=15)\n",
    "\n",
    "state_dict = torch.load(f\"{save_dir}\\kernel_model_weights_epoch_10.pth\", map_location=device)\n",
    "\n",
    "# remove the info from the last layer\n",
    "state_dict.pop('fc.weight', None)\n",
    "state_dict.pop('fc.bias', None)\n",
    "\n",
    "model = models.resnet18(pretrained=False)\n",
    "model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "# remove last layer so we can do feature extraction\n",
    "model = torch.nn.Sequential(*(list(model.children())[:-1]))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "train_features = []\n",
    "train_labels = []\n",
    "\n",
    "test_features = []\n",
    "test_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, lbls in tqdm(train_loader, desc=\"Extracting train features\"):\n",
    "        imgs = imgs.to(device)\n",
    "        feats = model(imgs).squeeze()\n",
    "        train_features.append(feats.cpu().numpy())\n",
    "        train_labels.extend(lbls.numpy())\n",
    "\n",
    "    for imgs, lbls in tqdm(test_loader, desc=\"Extracting test features\"):\n",
    "        imgs = imgs.to(device)\n",
    "        feats = model(imgs).squeeze()\n",
    "        test_features.append(feats.cpu().numpy())\n",
    "        test_labels.extend(lbls.numpy())\n",
    "\n",
    "train_features = np.vstack(train_features)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "test_features = np.vstack(test_features)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "print(\"Train feature matrix shape:\", train_features.shape)\n",
    "print(\"Test feature matrix shape:\", test_features.shape)\n",
    "\n",
    "np.save(f\"{image_size}train_features.npy\", train_features)\n",
    "np.save(f\"{image_size}train_labels.npy\", train_labels)\n",
    "\n",
    "np.save(f\"{image_size}test_features.npy\", test_features)\n",
    "np.save(f\"{image_size}test_labels.npy\", test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2859718b-1e51-43f1-b35c-ea8c33467014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import mrmr\n",
    "\n",
    "#Load data from disk\n",
    "image_size = \"(224, 224)\"\n",
    "train_features = np.load(f\"{image_size}train_features.npy\")\n",
    "train_labels   = np.load(f\"{image_size}train_labels.npy\")\n",
    "test_features  = np.load(f\"{image_size}test_features.npy\")\n",
    "test_labels    = np.load(f\"{image_size}test_labels.npy\")\n",
    "\n",
    "#Most of these models prefer scaled values between 0 and 1, so performing that here\n",
    "scaler = StandardScaler()\n",
    "train_scaled = scaler.fit_transform(train_features)\n",
    "test_scaled  = scaler.transform(test_features)\n",
    "\n",
    "#Stores each of the datasets and their corrosponding number of components\n",
    "datasets = {}\n",
    "components = {}\n",
    "\n",
    "datasets['Unreduced CNN Vector'] = (train_scaled, test_scaled)\n",
    "components['Unreduced CNN Vector'] = 512;\n",
    "\n",
    "#Creates the PCA dataset to account for 99% variance\n",
    "pca = PCA(n_components=0.99, random_state=42)\n",
    "train_pca = pca.fit_transform(train_scaled)\n",
    "test_pca  = pca.transform(test_scaled)\n",
    "datasets['PCA'] = (train_pca, test_pca)\n",
    "components['PCA'] = train_pca.shape[1]\n",
    "\n",
    "#Creates the LDA dataset\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "train_lda = lda.fit_transform(train_scaled, train_labels)\n",
    "test_lda  = lda.transform(test_scaled)\n",
    "datasets['LDA'] = (train_lda, test_lda)\n",
    "components['LDA'] = train_lda.shape[1]\n",
    "\n",
    "#Creates the MI + PCA set\n",
    "K = 300  # number of features to keep during MI. 300 worked the best through testing.\n",
    "df_train = pd.DataFrame(train_scaled)\n",
    "df_train['label'] = train_labels\n",
    "\n",
    "#MI is supervised, so it needs to take in the labels when evaluating\n",
    "selected_features = mrmr.mrmr_classif(\n",
    "    X=df_train.drop('label', axis=1),\n",
    "    y=df_train['label'],\n",
    "    K=K\n",
    ")\n",
    "train_mrmr = train_scaled[:, selected_features]\n",
    "test_mrmr  = test_scaled[:, selected_features]\n",
    "#The PCA step in MI + PCA. Once again keeping 99% variance\n",
    "pca_mrmr = PCA(n_components=0.99, random_state=42)\n",
    "train_mrmr_pca = pca_mrmr.fit_transform(train_mrmr)\n",
    "test_mrmr_pca  = pca_mrmr.transform(test_mrmr)\n",
    "datasets['MI+PCA'] = (train_mrmr_pca, test_mrmr_pca)\n",
    "components['MI+PCA'] = train_mrmr_pca.shape[1]\n",
    "\n",
    "#The list of classifiers we want to use\n",
    "classifier_list = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=500), #500 performed the best for accuracy\n",
    "    'SVM': SVC(kernel='rbf', C=50, gamma='scale'), #Using anything other than rbf and scale performed extremely poorly\n",
    "    'XGBoost': XGBClassifier( #This is a relatively small XGBoost tree, but making it larger didn't help\n",
    "        n_estimators=100, learning_rate=0.2, max_depth=6,\n",
    "        subsample=0.8, colsample_bytree=0.8, random_state=42,\n",
    "        use_label_encoder=False, eval_metric='logloss'\n",
    "    ),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5) #k=5\n",
    "}\n",
    "\n",
    "#Store the accuracies. We'll need them later\n",
    "accuracies = {key: [] for key in datasets.keys()}\n",
    "\n",
    "#Iterates thorugh every dataset within the sets created earlier, and runs each classifier on them.\n",
    "#Accuracies are stored for plotting later, and also the confusion matrices are printed\n",
    "for dataset_name, (X_train, X_test) in datasets.items():\n",
    "    for clf_name, clf in classifier_list.items():\n",
    "        clf.fit(X_train, train_labels)\n",
    "        acc = clf.score(X_test, test_labels)\n",
    "        accuracies[dataset_name].append(acc)\n",
    "\n",
    "        preds = clf.predict(X_test)\n",
    "\n",
    "        cm = confusion_matrix(test_labels, preds)\n",
    "        disp = ConfusionMatrixDisplay(cm)\n",
    "        disp.plot(cmap='Blues')\n",
    "        plt.title(dataset_name + \" with \" + clf_name + \"Confusion Matrix\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "#I like these colors :) \n",
    "colors = ['skyblue', 'salmon', 'lightgreen', 'plum']  # for the datasets\n",
    "control_accs = [0.80, 0.83]\n",
    "\n",
    "#The controls are hardcoded in later\n",
    "control_names = ['Control: Kaspersky', 'Control: ResNet18']\n",
    "control_colors = ['gold', 'orange']\n",
    "\n",
    "all_classifiers = list(classifier_list.keys()) + control_names\n",
    "x = np.arange(len(all_classifiers))\n",
    "width = 0.275\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "\n",
    "#THIS CREATES A VERY UGLY PLOT BY DEFAULT. \n",
    "#It will plot every single classifier and dataset on the same chart, and everything smooshes into each other.\n",
    "#If you want to examine an individual classifier, I reccomend commenting out / removing it from the classifier list. Everything looks horrible otherwise\n",
    "for i, key in enumerate(datasets.keys()):\n",
    "    ax.bar(x[:len(classifier_list)] + i*width - (len(datasets)-1)*width/2, \n",
    "           accuracies[key], width, label=key, color=colors[i])\n",
    "    \n",
    "    for j, acc in enumerate(accuracies[key]):\n",
    "        #puts the num components on top of the bar\n",
    "        ax.text(\n",
    "            x=j + i*width - (len(datasets)-1)*width/2,\n",
    "            y=0.75,  #hardcoded magic number. Puts the num of components nicely on the bar\n",
    "            s=str(components[key]),\n",
    "            ha='center',\n",
    "            va='center',\n",
    "            fontsize=9,\n",
    "            color='black'\n",
    "        )\n",
    "        #Puts the accuracy above the bar\n",
    "        ax.text(\n",
    "            x=j + i*width - (len(datasets)-1)*width/2,\n",
    "            y=acc + 0.005,\n",
    "            s=f\"{acc*100:.1f}%\",\n",
    "            ha='center',\n",
    "            va='bottom',\n",
    "            fontsize=9\n",
    "        )\n",
    "\n",
    "#This plots the hardcoded values for kaspersky and resnet\n",
    "for i, (name, acc) in enumerate(zip(control_names, control_accs)):\n",
    "    ax.bar(len(classifier_list) + i, acc, width, label=name, color=control_colors[i])\n",
    "    ax.text(len(classifier_list) + i, acc + 0.005, f\"{int(acc*100)}%\", \n",
    "            ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "#X labels\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(all_classifiers)\n",
    "\n",
    "#Y labels and legend\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_xlabel('Classifier')\n",
    "ax.set_title('Classifier Performance Across Different Dimension Reduction Methods')\n",
    "ax.set_ylim(0.5, 1.05)\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "ax.legend(title='Dataset', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
